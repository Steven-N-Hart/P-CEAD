{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proganomaly Polygon Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "TensorFlow and numpy should already be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get update\n",
    "pip3 install --upgrade pip\n",
    "pip3 install scikit-image\n",
    "pip3 install shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs\n",
    "\n",
    "There are four main groups of configs: input, output, polygon, and Dataflow.\n",
    "\n",
    "The input configs are where input files are located. Annotations and KDE grayscale images are converted to Shapely `MultiPolygon`s. The patch coordinates are used to create a `MultiPolygon` of all of the patches as a outer bound of the prediction `MultiPolygon`.\n",
    "\n",
    "The output config contains a boolean flag whether to use Dataflow or not and the output GCS path to write the polygon confusion matrix results.\n",
    "\n",
    "The polygon config contains all hyperparamters needed for making polygons.\n",
    "\n",
    "The dataflow config contains the parameters needed for the Dataflow polygon pipeline, only used if Dataflow gets called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_config = {\n",
    "    \"slide_name\": \"slide_name\",\n",
    "    \"annotations_image_gcs_path\": \"gs://.../{slide_name}_annotations.png\",\n",
    "    \"kde_gs_image_gcs_path\": \"gs://.../_kde_gs.png\",\n",
    "    \"patch_coordinates_gcs_path\": \"gs://.../_patch_coordinates.csv-00000-of-00001\",    \n",
    "}\n",
    "output_config = {\n",
    "    \"use_dataflow\": True,\n",
    "    \"output_gcs_path\": \"gs://.../\"\n",
    "}\n",
    "polygon_config = {\n",
    "    # Height in pixels of a patch.\n",
    "    \"patch_height\": 1024,\n",
    "    # Width in pixels of a patch.\n",
    "    \"patch_width\": 1024,\n",
    "    # Height in pixels of input image.\n",
    "    \"image_height\": 8192,\n",
    "    # Width in pixels of input image.\n",
    "    \"image_width\": 8192,\n",
    "    # Let's say you have a slide that is 86000 x 112000. This means, if my\n",
    "    # patches are 1024 x1024, that 83.984 ~ 83 patches can fit in the x\n",
    "    # dimension and 109.375 ~ 109 patches can fit in the y dimension. However,\n",
    "    # I need to stitch cleanly a left and a right patch (power of 2 in the x\n",
    "    # dimension) and an up and a down patch (power of 2 in the y dimension).\n",
    "    # Therefore the next closest biggest power of 2 in the x dimension is\n",
    "    # 83 -> 128 and in the y is 109 -> 128. This results in a 128 x 128 patch\n",
    "    # image. Even though this is already square, in case it is not, we take\n",
    "    # the max of each dimension and then set both to that.\n",
    "\n",
    "    # log(128, 2) = 7. That is where the 7 comes from. The stitching will\n",
    "    # require a depth of 7 of the 4-ary tree to complete the slide.\n",
    "    # Depth: 7, Size 128x128\n",
    "    # Depth: 6, Size 64x64\n",
    "    # Depth: 5, Size 32x32\n",
    "    # Depth: 4, Size 16x16\n",
    "    # Depth: 3, Size 8x8\n",
    "    # Depth: 2, Size 4x4\n",
    "    # Depth: 1, Size 2x2\n",
    "    # Depth: 0, Size 1x1\n",
    "\n",
    "    # I need to be able to reconstruct that size.\n",
    "    # Therefore, (num_patches) * (patch_size) = (2 ** 7) * (1024)\n",
    "    \"effective_slide_height\": 2 ** 7 * 1024,\n",
    "    \"effective_slide_width\": 2 ** 7 * 1024,\n",
    "    # Max number of seconds to wait for each element to complete. This is\n",
    "    # helps Dataflow jobs from getting stuck and failing due to straggelers.\n",
    "    # To disable, set to 0.\n",
    "    \"timeout\": 600,\n",
    "    # List of thresholds to apply to KDE grayscale image to use result to\n",
    "    # create Polygons.\n",
    "    \"thresholds\": [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25],\n",
    "    # List of dilation factors to scale Polygons.\n",
    "    \"dilation_factors\": [1.0, 1.1, 1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 15.0, 20.0, 25.0, 30.0]\n",
    "}\n",
    "dataflow_config = {\n",
    "    # Project to run the Dataflow job .\n",
    "    \"project\": \"...\",\n",
    "    # GCS bucket to stage temporary files.\n",
    "    \"bucket\": \"gs://...\",\n",
    "    # Region to run the Dataflow job, make sure you have quota.\n",
    "    \"region\": \"us-central1\",\n",
    "    # Autoscaling mode for Dataflow job. Possible values are THROUGHPUT_BASED\n",
    "    # to enable autoscaling or NONE to disable.\n",
    "    \"autoscaling_algorithm\": \"NONE\",\n",
    "    # Initial number of Google Compute Engine instances to use when executing\n",
    "    # your pipeline. This option determines how many workers the Dataflow\n",
    "    # service starts up when your job begins.\n",
    "    \"num_workers\": 60,\n",
    "    # Compute Engine machine type that Dataflow uses when starting worker VMs.\n",
    "    \"machine_type\": \"n1-highmem-32\",\n",
    "    # Disk size, in gigabytes, to use on each remote Compute Engine worker instance.\n",
    "    \"disk_size_gb\": 1000,\n",
    "    # Specifies a user-managed controller service account, using the format\n",
    "    # my-service-account-name@<project-id>.iam.gserviceaccount.com.\n",
    "    \"service_account_email\": \"...\",\n",
    "    # Specifies whether Dataflow workers use public IP addresses. If the value\n",
    "    # is set to false, Dataflow workers use private IP addresses for all\n",
    "    # communication. In this case, if the subnetwork option is specified, the\n",
    "    # network option is ignored. Make sure that the specified network or\n",
    "    # subnetwork has Private Google Access enabled. Public IP addresses have\n",
    "    # an associated cost.\n",
    "    \"use_public_ips\": False,\n",
    "    # Compute Engine network for launching Compute Engine instances to run\n",
    "    # your pipeline.\n",
    "    \"network\": \"https://...\",\n",
    "    # Compute Engine subnetwork for launching Compute Engine instances to run\n",
    "    # your pipeline.\n",
    "    \"subnetwork\": \"https://...\",\n",
    "    # Runner of pipeline. \"DirectRunner\" for running local, \"DataflowRunner\"\n",
    "    # for running distributed Dataflow job.\n",
    "    \"runner\": \"DataflowRunner\"  # Directrunner or DataflowRunner\n",
    "}\n",
    "config = {\n",
    "    \"input\": input_config,\n",
    "    \"output\": output_config,\n",
    "    \"polygon\": polygon_config,\n",
    "    \"dataflow\": dataflow_config\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon Confusion Matrix Pipeline\n",
    "\n",
    "This is the entrypoint code for the polygon confusion matrix pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from proganomaly_modules.inference_module import polygon_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline\n",
    "\n",
    "We can now run the polygon confusion matrix pipeline with our configs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_confusion_matrix.save_polygon_confusion_matrices(config)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
