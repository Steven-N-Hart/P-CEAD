{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proganomly Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "TensorFlow and numpy should already be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get update\n",
    "sudo apt-get --assume-yes install openslide-tools\n",
    "sudo apt-get --assume-yes install python-openslide\n",
    "pip3 install --upgrade pip\n",
    "pip3 install opencv-python-headless\n",
    "pip3 install openslide-python\n",
    "pip3 install matplotlib\n",
    "pip3 install scikit-image\n",
    "pip3 install shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs\n",
    "\n",
    "There are six main groups of configs: input, output, GAN inference, segmentation inference, polygon inference, and Dataflow.\n",
    "\n",
    "The input configs are where input files are located. As requested there are several possible paths.\n",
    "\n",
    "1. If `pre_computed_image_gcs_path` is not empty, inference from the model will be skipped and images at that GCS path will be read and used to create polygons.\n",
    "1. If `png_individual_gcs_glob_pattern` is not empty, query image patch files will be read from GCS using this glob pattern and will call the model for inference and will create polygons at the end. To increase speed of inference, we can copy the SavedModel to a local directory and set `export_dir` to that path. Don't forget to then change `exports_on_gcs` to `True`.\n",
    "1. If `png_patch_stitch_gcs_path` or `wsi_stitch_gcs_path` is not empty then the Dataflow stitching pipeline will be called where all of the patches will call the model for inference and then be stitched together. Since this can take quite some time, the notebook may lose connection, despite the Dataflow job still running perfectly in the background. If this happens, then change `pre_computed_image_gcs_path` to be what `gcs_output_image_filepattern` was previously to use the results of the Dataflow pipeline. `png_patch_stitch_gcs_path` and `wsi_stitch_gcs_path` cannot both be set at the same time.\n",
    "  1. If `png_patch_stitch_gcs_path` is not empty, then the Dataflow stitching pipeline will use pre-extracted patch PNG files at the GCS path for inference and then stitch after.\n",
    "  1. If `wsi_stitch_gcs_path` is not empty, then the Dataflow stitching pipeline will use the WSI at that path to extract patches in memory and then stitch after inference.\n",
    "\n",
    "The output config contains boolean flags for which output types are desired as well as the GCS output path `gcs_output_image_filepattern`.\n",
    "\n",
    "The GAN inference config contains all hyperparamters needed for GAN inference.\n",
    "\n",
    "The segmentation inference config contains all hyperparamters needed for segmentation inference.\n",
    "\n",
    "The polygon inference config contains all hyperparamters needed for polygon inference.\n",
    "\n",
    "The dataflow config contains the parameters needed for the Dataflow sitching pipeline, only used if Dataflow gets called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_config = {\n",
    "    \"slide_name\": \"slide_name\",\n",
    "    \"use_pre_computed_gcs_paths\": True,\n",
    "    # berg only.\n",
    "#     \"pre_computed_generated_images_gcs_path\": \"\",\n",
    "#     \"pre_computed_encoded_generated_images_gcs_path\": \"\",\n",
    "#     \"pre_computed_query_encoded_images_gcs_path\": \"\",\n",
    "    # GANomaly only.\n",
    "#     \"pre_computed_query_gen_encoded_images_gcs_path\": \"\",\n",
    "    # Both.\n",
    "#     \"pre_computed_query_images_gcs_path\": \"\",\n",
    "#     \"pre_computed_query_anomaly_images_linear_rgb_gcs_path\": \"\",\n",
    "#     \"pre_computed_query_anomaly_images_linear_gs_gcs_path\": \"\",\n",
    "#     \"pre_computed_query_mahalanobis_distance_images_linear_gcs_path\": \"\",\n",
    "#     \"pre_computed_query_pixel_anomaly_flag_images_gcs_path\": \"\",\n",
    "#     \"pre_computed_kde_rgb_gcs_path\": \"\",\n",
    "#     \"pre_computed_kde_gs_gcs_path\": \"\",\n",
    "#     \"pre_computed_kde_gs_thresholded_gcs_path\": \"\",\n",
    "#     \"pre_computed_segmentation_cell_coords_gcs_path\": \"\",\n",
    "#     \"pre_computed_segmentation_nuclei_coords_gcs_path\": \"\",\n",
    "#     \"pre_computed_patch_coordinates_gcs_path\": \"\",\n",
    "\n",
    "    \"png_individual_gcs_glob_pattern\": \"\",\n",
    "    \"png_patch_stitch_gcs_glob_pattern\": \"\",\n",
    "    \"wsi_stitch_gcs_path\": \"\"\n",
    "}\n",
    "output_config = {\n",
    "    # berg only.\n",
    "    \"output_generated_images\": False,\n",
    "    \"output_encoded_generated_images\": False,\n",
    "    \"output_query_encoded_images\": False,\n",
    "    # GANomaly only.\n",
    "    \"output_query_gen_encoded_images\": False,\n",
    "    # Both.\n",
    "    \"output_query_images\": False,\n",
    "    \"output_query_anomaly_images_linear_rgb\": False,\n",
    "    \"output_query_anomaly_images_linear_gs\": False,\n",
    "    \"output_query_mahalanobis_distance_images_linear\": False,\n",
    "    \"output_query_pixel_anomaly_flag_images\": False,\n",
    "    \"output_kde_rgb\": False,\n",
    "    \"output_kde_gs\": False,\n",
    "    \"output_kde_gs_thresholded\": False,\n",
    "    # Requires query_images, kde_gs, and patch_coordinates.\n",
    "    \"output_kde_gs_polygon\": True,\n",
    "    \"output_segmentation_cell_coords\": False,\n",
    "    \"output_segmentation_nuclei_coords\": False,\n",
    "    \"output_patch_coordinates\": False,\n",
    "    # Used if running a beam stitch job. Don't forget the slash on the end.\n",
    "    \"output_gcs_path\": \"gs://.../test/\"\n",
    "}\n",
    "gan_inference_config = {\n",
    "    # Approximate width of resultant thumbnail image.\n",
    "    \"target_image_width\": 500,\n",
    "    # Method to use for converting thumbnail of slide into binary mask. Either\n",
    "    # otsu or rgb2hed.\n",
    "    \"thumbnail_method\": \"otsu\",\n",
    "    # Threshold to convert RGB2HED image to binary mask.\n",
    "    \"rgb2hed_threshold\": -0.41,\n",
    "    # Threshold to compare with percent of binary flags within a patch region\n",
    "    # to include in collection.\n",
    "    \"include_patch_threshold\": 0.5,\n",
    "    # Whether SavedModel exports are on GCS or local.\n",
    "    \"exports_on_gcs\": True,\n",
    "#     \"exports_on_gcs\": False,\n",
    "    # GAN SavedModel export directory on GCS or local filesystem.\n",
    "    \"gan_export_dir\": \"gs://.../trained_models\",\n",
    "#     \"gan_export_dir\": \"../../trained_models\",\n",
    "    # Folder name of exported GAN SavedModel.\n",
    "    \"gan_export_name\": \"dynamic_threshold\",\n",
    "    # Generator architecture type, 'berg' or 'GANomaly'.\n",
    "    \"generator_architecture\": \"GANomaly\",\n",
    "    # For berg architecture, whether to use Z inputs. Query image inputs are\n",
    "    # always used.\n",
    "    \"berg_use_Z_inputs\": True,\n",
    "    # For berg architecture, the latent size of the noise vector.\n",
    "    \"berg_latent_size\": 512,\n",
    "    # For berg architecture, the latent vector's random normal mean.\n",
    "    \"berg_latent_mean\": 0.0,\n",
    "    # For berg architecture, the latent vector's random normal standard\n",
    "    # deviation.\n",
    "    \"berg_latent_stddev\": 1.0,\n",
    "    # Bandwidth of the kernel.\n",
    "    \"bandwidth\": 100.0,\n",
    "    # Kernel to use for density estimation.\n",
    "    \"kernel\": \"gaussian\",\n",
    "    # Distance metric to use. Note that not all metrics are valid with all\n",
    "    # algorithms.\n",
    "    \"metric\": \"euclidean\",\n",
    "    # Number of sample bins to create in the x dimension.\n",
    "    \"xbins\": 100,\n",
    "    # Number of sample bins to create in the y dimension.\n",
    "    \"ybins\": 100,\n",
    "    # Minimum number of adjacent points as not to be removed from image.\n",
    "    \"min_neighborhood_count\": 50,\n",
    "    # Connectivity defining the neighborhood of a pixel.\n",
    "    \"connectivity\": 10,\n",
    "    # Minimum number of anomaly points following removing small objects to not\n",
    "    # clear all flags.\n",
    "    \"min_anomaly_points_remaining\": 200,\n",
    "    # Exponent to use for scaling.\n",
    "    \"scaling_power\": 0.5,\n",
    "    # Positive factor to scale anomaly flag counts by.\n",
    "    \"scaling_factor\": 100000.0,\n",
    "    # Which color map to use.\n",
    "    \"cmap_str\": \"turbo\",\n",
    "    # Amount to scale the bandwidth based on anomaly counts. If 0.0, no scaling.\n",
    "    \"dynamic_bandwidth_scale_factor\": 50000.0,\n",
    "    # Maximum number of points allowed to run KDE. Otherwise entire image is marked as anomalous.\n",
    "    \"max_anomaly_points_for_kde\": 50000,\n",
    "    # Threshold to convert KDE grayscale image into binary mask to create image.\n",
    "    \"kde_threshold\": 0.2,\n",
    "    # Threshold to override learned Mahalanobis distance threshold from\n",
    "    # SavedModel for creating Mahalanobis binary mask.\n",
    "    \"custom_mahalanobis_distance_threshold\": 2.8172882 + 3.0 * 67.66342,  # mu + num_sigma * stddev\n",
    "    # Depth of n-ary tree for GAN image stitching.\n",
    "    # Let's say you have a slide that is 86000 x 112000. This means, if my\n",
    "    # patches are 1024 x1024, that 83.984 ~ 83 patches can fit in the x\n",
    "    # dimension and 109.375 ~ 109 patches can fit in the y dimension. However,\n",
    "    # I need to stitch cleanly a left and a right patch (power of 2 in the x\n",
    "    # dimension) and an up and a down patch (power of 2 in the y dimension).\n",
    "    # Therefore the next closest biggest power of 2 in the x dimension is\n",
    "    # 83 -> 128 and in the y is 109 -> 128. This results in a 128 x 128 patch\n",
    "    # image. Even though this is already square, in case it is not, we take\n",
    "    # the max of each dimension and then set both to that.\n",
    "\n",
    "    # log(128, 2) = 7. That is where the 7 comes from. The stitching will\n",
    "    # require a depth of 7 of the 4-ary tree to complete the slide.\n",
    "    # Depth: 7, Size 128x128\n",
    "    # Depth: 6, Size 64x64\n",
    "    # Depth: 5, Size 32x32\n",
    "    # Depth: 4, Size 16x16\n",
    "    # Depth: 3, Size 8x8\n",
    "    # Depth: 2, Size 4x4\n",
    "    # Depth: 1, Size 2x2\n",
    "    # Depth: 0, Size 1x1\n",
    "    \"nary_tree_depth\": 7  # most use 7, 1805.WT uses 8, 41.BRAF_V600E uses 6\n",
    "    # List of 2-tuples of output image height and width for each n-ary level, starting from leaves.\n",
    "    \"output_image_sizes\": [(1024, 1024)] * 10,\n",
    "}\n",
    "# Segmentation inference config only used if outputting:\n",
    "# output_segmentation_cell_coords or output_segmentation_nuclei_coords.\n",
    "segmentation_inference_config = {\n",
    "    # Directory containing exported segmentation models.\n",
    "    \"segmentation_export_dir\": \"gs://.../nuclei_segmentation_model\",\n",
    "    # Name of segmentation model\n",
    "    \"segmentation_model_name\": \"segmentation_model_name.meta\",\n",
    "    # Size of each patch of image for segmentation model.\n",
    "    \"segmentation_patch_size\": 128,\n",
    "    # Number of pixels to skip for each patch of image for segmentation model.\n",
    "    \"segmentation_stride\": 16,\n",
    "    # Whether to median blur images before segmentation.\n",
    "    \"segmentation_median_blur_image\": False,\n",
    "    # Kernel size of median blur for segmentation.\n",
    "    \"segmentation_median_blur_kernel_size\": 9,\n",
    "    # Number of patches to include in a group for segmentation.\n",
    "    \"segmentation_group_size\": 10\n",
    "}\n",
    "inference_config = {\n",
    "    # Number of images to include in each batch for inference.\n",
    "    \"batch_size\": 8,\n",
    "    # Height in pixels of a patch.\n",
    "    \"patch_height\": 1024,\n",
    "    # Width in pixels of a patch.\n",
    "    \"patch_width\": 1024,\n",
    "    # Number of channels of an image patch.\n",
    "    \"patch_depth\": 3,\n",
    "    \"gan\": gan_inference_config,\n",
    "    \"segmentation\": segmentation_inference_config\n",
    "}\n",
    "polygon_config = {\n",
    "    # Threshold to convert KDE grayscale image into binary mask to create\n",
    "    # Polygon.\n",
    "    \"kde_gs_polygon_threshold\": 0.2,\n",
    "    # Factor to scale/dilate the `MultiPolygon`.\n",
    "    \"kde_gs_polygon_dilation_factor\": 1.0,\n",
    "    # The origin each polygon should be scaled about. 'center' or 'centroid'.\n",
    "    \"dilation_origin\": \"centroid\",\n",
    "    # Whether to limit Polygon vertices to only lie within patches. Requires\n",
    "    # patch coordinates list. For stitched slide images, set to True.\n",
    "    # Otherwise, for individual PNG images, set to False.\n",
    "    \"limit_polygon_vertices_to_only_patches\": True,\n",
    "    # Let's say you have a slide that is 86000 x 112000. This means, if my\n",
    "    # patches are 1024 x1024, that 83.984 ~ 83 patches can fit in the x\n",
    "    # dimension and 109.375 ~ 109 patches can fit in the y dimension. However,\n",
    "    # I need to stitch cleanly a left and a right patch (power of 2 in the x\n",
    "    # dimension) and an up and a down patch (power of 2 in the y dimension).\n",
    "    # Therefore the next closest biggest power of 2 in the x dimension is\n",
    "    # 83 -> 128 and in the y is 109 -> 128. This results in a 128 x 128 patch\n",
    "    # image. Even though this is already square, in case it is not, we take\n",
    "    # the max of each dimension and then set both to that.\n",
    "\n",
    "    # log(128, 2) = 7. That is where the 7 comes from. The stitching will\n",
    "    # require a depth of 7 of the 4-ary tree to complete the slide.\n",
    "    # Depth: 7, Size 128x128\n",
    "    # Depth: 6, Size 64x64\n",
    "    # Depth: 5, Size 32x32\n",
    "    # Depth: 4, Size 16x16\n",
    "    # Depth: 3, Size 8x8\n",
    "    # Depth: 2, Size 4x4\n",
    "    # Depth: 1, Size 2x2\n",
    "    # Depth: 0, Size 1x1\n",
    "\n",
    "    # I need to be able to reconstruct that size.\n",
    "    # Therefore, (num_patches) * (patch_size) = (2 ** 7) * (1024)\n",
    "    \"effective_slide_height\": 2 ** 7 * 1024,\n",
    "    \"effective_slide_width\": 2 ** 7 * 1024\n",
    "}\n",
    "# Dataflow config only used if calling a Dataflow stitch job.\n",
    "dataflow_config = {\n",
    "    # Project to run the Dataflow job .\n",
    "    \"project\": \"...\",\n",
    "    # GCS bucket to stage temporary files.\n",
    "    \"bucket\": \"gs://...\",\n",
    "    # Region to run the Dataflow job, make sure you have quota.\n",
    "    \"region\": \"us-central1\",\n",
    "    # Autoscaling mode for Dataflow job. Possible values are THROUGHPUT_BASED\n",
    "    # to enable autoscaling or NONE to disable.\n",
    "    \"autoscaling_algorithm\": \"NONE\",\n",
    "    # Initial number of Google Compute Engine instances to use when executing\n",
    "    # your pipeline. This option determines how many workers the Dataflow\n",
    "    # service starts up when your job begins.\n",
    "    \"num_workers\": 60,\n",
    "    # Compute Engine machine type that Dataflow uses when starting worker VMs.\n",
    "    \"machine_type\": \"n1-highmem-32\",\n",
    "    # Disk size, in gigabytes, to use on each remote Compute Engine worker instance.\n",
    "    \"disk_size_gb\": 1000,\n",
    "    # Specifies a user-managed controller service account, using the format\n",
    "    # my-service-account-name@<project-id>.iam.gserviceaccount.com.\n",
    "    \"service_account_email\": \"...\",\n",
    "    # Specifies whether Dataflow workers use public IP addresses. If the value\n",
    "    # is set to false, Dataflow workers use private IP addresses for all\n",
    "    # communication. In this case, if the subnetwork option is specified, the\n",
    "    # network option is ignored. Make sure that the specified network or\n",
    "    # subnetwork has Private Google Access enabled. Public IP addresses have\n",
    "    # an associated cost.\n",
    "    \"use_public_ips\": False,\n",
    "    # Compute Engine network for launching Compute Engine instances to run\n",
    "    # your pipeline.\n",
    "    \"network\": \"https://...\",\n",
    "    # Compute Engine subnetwork for launching Compute Engine instances to run\n",
    "    # your pipeline.\n",
    "    \"subnetwork\": \"https://...\",\n",
    "    # Runner of pipeline. \"DirectRunner\" for running local, \"DataflowRunner\"\n",
    "    # for running distributed Dataflow job.\n",
    "    \"runner\": \"DataflowRunner\"  # Directrunner or DataflowRunner\n",
    "}\n",
    "config = {\n",
    "    \"input\": input_config,\n",
    "    \"output\": output_config,\n",
    "    \"inference\": inference_config,\n",
    "    \"polygon\": polygon_config,\n",
    "    \"dataflow\": dataflow_config\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Pipeline\n",
    "\n",
    "This is the entrypoint code for the inference pipeline. As mentioned above there are many conditional paths depending on what was set in the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from proganomaly_modules.inference_module import inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline\n",
    "\n",
    "We can now run the inference pipeline with our configs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = inference.inference_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# ./run_beam.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Outputs\n",
    "\n",
    "With our pipeline run, we can now inspect the output dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proganomaly_modules.inference_module import image_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_utils.plot_images(\n",
    "    images=image_utils.descale_images(images=output_dict[\"query_images\"]),\n",
    "    depth=3,\n",
    "    num_rows=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_utils.plot_images(\n",
    "    images=image_utils.descale_images(images=output_dict[\"kde_rgb\"]),\n",
    "    depth=3,\n",
    "    num_rows=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_utils.plot_images(\n",
    "    images=image_utils.descale_images(images=output_dict[\"kde_gs\"]),\n",
    "    depth=1,\n",
    "    num_rows=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, polygon in enumerate(output_dict[\"kde_gs_polygon\"]):\n",
    "    image_utils.plot_geometry_contours(\n",
    "        geometry=polygon,\n",
    "        image=output_dict[\"query_images\"][i],\n",
    "        fig_name=\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thresh in [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.5]:\n",
    "    for dil in [4.0]:\n",
    "        config[\"polygon\"][\"kde_gs_polygon_threshold\"] = thresh\n",
    "        config[\"polygon\"][\"kde_gs_polygon_dilation_factor\"] = dil\n",
    "        output_dict = inference.inference_pipeline(config)\n",
    "        image_utils.plot_geometry_contours(\n",
    "            geometry=output_dict[\"kde_gs_polygon\"][0],\n",
    "            image=output_dict[\"query_images\"][0],\n",
    "            fig_name=\"18.WT_MHD3.0_KDEt{thresh}_DIL{dil}_MINPIX500\".format(\n",
    "                thresh=thresh,dil=dil\n",
    "            )\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
